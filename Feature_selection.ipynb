{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronygsp/ImplementedDataStructures/blob/main/Feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is Feature Selection?**\n",
        "\n",
        "In the realm of machine learning, feature selection is the process of identifying and selecting a subset of relevant features for use in model construction. This process is crucial because the quality and quantity of features feeding into a model significantly impact its performance and interpretability. By choosing the right features, we can improve model accuracy, reduce complexity, and increase computational efficiency.\n",
        "\n",
        "Imagine you're trying to predict the quality of wine based on various attributes (features) like acidity, sugar level, alcohol content, etc. Not all these attributes might be relevant to determining wine quality. Feature selection helps us identify which attributes (features) are essential and which can be ignored, thus simplifying our model without sacrificing accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "_lzn3Lz6GBax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why is Feature Selection Important?**\n",
        "\n",
        "Feature selection is not just about finding the needle in the haystack. It's about recognizing that not all data points are created equal when it comes to the predictive power they hold. Here are a few reasons why feature selection is vital:\n",
        "\n",
        "1. **Improves Model Accuracy:** Eliminates noise by removing irrelevant or redundant features, thereby enhancing the model's predictive performance.\n",
        "2. **Reduces Overfitting:** Fewer irrelevant features reduce the chance of making decisions based on noise, leading to a more generalized model.\n",
        "3. **Increases Model Efficiency:** Less data means faster training. Models train quicker on fewer features.\n",
        "4. **Enhances Model Interpretability:** A model with fewer features is easier to understand and explain, making the outcomes more actionable."
      ],
      "metadata": {
        "id": "aqxOxeG3JCFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Three Main Approaches to Feature Selection**\n",
        "\n",
        "Feature selection techniques are broadly classified into three categories: **Filter Methods, Wrapper Methods, and Embedded Methods**. Each has its own merits and fits different scenarios."
      ],
      "metadata": {
        "id": "MFzSJjeOHXwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filter Methods: Unveiling Patterns through Statistical Scores**\n",
        "\n",
        "Filter methods are a class of feature selection techniques that operate independently of any machine learning algorithm. Instead, they evaluate each feature based on certain statistical measures, and features are selected or discarded before introducing the data to the learning algorithm. This approach offers simplicity, speed, and efficiency in identifying relevant features. Let's explore some key aspects of filter methods:\n",
        "\n",
        "1. **Statistical Scores:**\n",
        " * **Idea:** Filter methods rank features by their statistical scores, which measure the correlation or association between each feature and the target variable.\n",
        " * **Score (metrics):** Filter methods employ various statistical metrics, such as correlation coefficients, chi-square statistics, information gain, or mutual information, depending on the nature of the data (continuous or categorical) and the task at hand.\n",
        "2. **Independence from Algorithms:**\n",
        " * **Advantage:** Filter methods are algorithm-agnostic. They assess features independently of the machine learning model that will eventually be applied. This makes them computationally less expensive and a suitable starting point for feature selection.\n",
        "3. **Feature Ranking:**\n",
        " * **Process:** Features are ranked based on their individual scores. Those with higher scores are deemed more relevant, while lower-ranked features are considered less informative for predicting the target variable.\n",
        "4. **Selection Criteria:**\n",
        " * **Thresholding:** A threshold is set, and features surpassing this threshold are retained, forming the selected subset.\n",
        "5. **Advantages of Filter Methods:**\n",
        " * **Computational Efficiency:** Filter methods are faster compared to wrapper methods and embedded methods, making them suitable for datasets with a large number of features.\n",
        " * **Algorithm Independence:** Since they don't rely on a specific learning algorithm, filter methods are versatile and can be applied to various types of datasets and models.\n",
        "6. **Disadvantages of Filter Methods:**\n",
        " * **Ignoring Feature Interactions:** Filter methods assess features in isolation and might overlook complex interactions between features. This limitation makes them less suitable for tasks where feature dependencies are crucial.\n",
        "7. **Common Scores:**\n",
        " * **Correlation Coefficient:** Measures linear relationships between continuous features and the target variable.\n",
        " * **Chi-Square Test:** Appropriate for categorical features, it assesses the independence of a feature from the target variable.\n",
        " * **Mutual Information:** Measures the dependence between two variables and is suitable for both continuous and categorical features."
      ],
      "metadata": {
        "id": "f7qyJmE4H-r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wrapper Methods: Iterative Search for Optimal Feature Sets**\n",
        "\n",
        "Wrapper methods for feature selection treat the process as a search problem, involving the evaluation of different subsets of features using a predictive model. Unlike filter methods, wrapper methods actively involve a machine learning algorithm in the feature selection process. This iterative approach seeks to find the optimal set of features by training and evaluating the model multiple times. Here are the key aspects of wrapper methods:\n",
        "\n",
        "1. **Search Strategies:**\n",
        "\n",
        " * **Idea:** Wrapper methods treat feature selection as a search problem, exploring different combinations of features to identify the subset that maximizes the performance of a given model.\n",
        "\n",
        " * **Examples:** Common search strategies include forward selection, backward elimination, and recursive feature elimination (RFE).\n",
        "\n",
        "2. **Model Evaluation:**\n",
        " * **Criteria:** A predictive model is used to evaluate each subset of features, and a performance metric (e.g., accuracy, F1 score) is assigned to each combination.\n",
        " * **Scoring:** The quality of a feature subset is determined by the model's predictive performance on a validation set.\n",
        "3. **Iterative Process:**\n",
        " * **Cycle:** Wrapper methods follow an iterative cycle of feature subset evaluation, model training, and refinement.\n",
        " * **Optimization:** The process continues until an optimal feature subset is found or until a specified stopping criterion is met.\n",
        "4. **Feature Ranking:**\n",
        " * **Dynamic:** Features are dynamically selected or excluded in each iteration, allowing the algorithm to explore the entire feature space.\n",
        "5. **Advantages of Wrapper Methods:**\n",
        " * **Model Sensitivity:** Wrapper methods consider the specific model's sensitivity to feature subsets, making them more suitable for complex relationships and interactions between features.\n",
        "\n",
        "6. **Disadvantages of Wrapper Methods:**\n",
        " * **Computational Cost:** Wrapper methods are computationally more expensive than filter methods, as they require training the model multiple times.\n",
        " * **Overfitting Risk:** There's a risk of overfitting to the training data, especially if the dataset is small, due to multiple evaluations.\n",
        "7. **Common Wrapper Methods:**\n",
        " * **Forward Selection:** Features are added one at a time, and at each step, the feature that provides the most improvement is included.\n",
        " * **Backward Elimination:** Starts with all features and removes the least significant one in each iteration until the specified stopping criterion is met.\n",
        " * **Recursive Feature Elimination (RFE)**: An iterative process where the least important features are pruned until the desired number of features is reached."
      ],
      "metadata": {
        "id": "FphNVVbITg8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedded Methods: Inherent Feature Selection within Model Training**\n",
        "\n",
        "Embedded methods seamlessly incorporate feature selection into the process of model training. Unlike filter and wrapper methods, embedded methods select features as a natural part of the algorithm's learning process. By doing so, they leverage the inherent capacity of certain algorithms to identify the most informative features. Here are the key aspects of embedded methods:\n",
        "\n",
        "1. **Integration with Model Training:**\n",
        " * **Inherent Selection:** Features are selected or assigned weights during the model training process itself.\n",
        " * **Learning Process:** The algorithm optimizes both the model parameters and the importance of each feature simultaneously.\n",
        "\n",
        "2. **Advantages of Embedded Methods:**\n",
        "  * **Inherent Feature Interactions:** Embedded methods naturally consider interactions between features as part of the learning process, making them effective for capturing complex relationships.\n",
        "  * **Model-Specific Selection:** Selection is tailored to the characteristics of the chosen model, ensuring a well-fitted subset of features.\n",
        "3. **Disadvantages of Embedded Methods:**\n",
        " * **Computational Cost:** Some embedded methods, especially those involving iterative optimization, can be computationally expensive.\n",
        " * **Algorithm Dependency:** The effectiveness of embedded methods depends on the suitability of the chosen model for the specific dataset.\n",
        "6. **Common Embedded Methods:**\n",
        " * **LASSO (Least Absolute Shrinkage and Selection Operator)**: Regularizes the linear regression model by adding a penalty term based on the absolute sum of feature coefficients.\n",
        " * **Tree-Based Models (Random Forest, Gradient Boosting):** Assigns importance scores to each feature during the ensemble learning process."
      ],
      "metadata": {
        "id": "w_quSJ-Ka24H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature selection with scikit-learn for the Breast Cancer Dataset**\n",
        "The Breast Cancer dataset from scikit-learn is a widely used dataset for binary classification tasks, particularly in the context of cancer diagnosis. It consists of features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. These features describe various characteristics of cell nuclei present in the images, such as radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n",
        "\n",
        "Here's more detailed information about the Breast Cancer dataset:\n",
        "\n",
        "* **Dataset Origin:** The dataset is derived from the Wisconsin Diagnostic Breast Cancer (WDBC) database available at https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic\n",
        "\n",
        "* **Number of Instances:** There are 569 instances, each corresponding to a breast cancer biopsy.\n",
        "Number of Features: The dataset comprises 30 features, representing measurements extracted from digitized images.\n",
        "\n",
        "* **Target Variable:** The target variable is binary, indicating whether the tumor is malignant (class 0) or benign (class 1).\n",
        "\n",
        "* **Use Case:** The primary use case for this dataset is binary classification, where the goal is to predict whether a tumor is malignant or benign based on the extracted features.\n",
        "\n",
        "* **Data Availability:** This dataset is included in the scikit-learn library, making it easily accessible for educational and research purposes.\n",
        "\n",
        "* **Research Significance:** Due to its real-world medical context and the critical nature of breast cancer diagnosis, this dataset is frequently employed for exploring machine learning techniques for cancer classification. It provides a challenging yet realistic task for building predictive models.\n",
        "\n",
        "In the context of your feature selection notebook, using the Breast Cancer dataset serves as a meaningful and relevant example. The task of predicting cancer malignancy based on various features underscores the importance of feature selection in enhancing the interpretability and performance of machine learning models for critical applications in healthcare and diagnostics."
      ],
      "metadata": {
        "id": "2OK9Ck0mNRrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline model using all available variables**\n",
        "In our exploration of feature selection, we begin by loading and preparing the Breast Cancer dataset, a well-known benchmark in machine learning. This dataset encompasses various features related to breast cancer characteristics. To ensure robust model evaluation, we strategically split the dataset into training and testing sets. Subsequently, we standardize the features to bring them to a common scale, a crucial preprocessing step that aids in the fair comparison of their contributions to the model. With our data prepared, we proceed to implement and assess a baseline model, utilizing all available features. This initial model serves as our reference point, providing insights into the dataset's inherent characteristics and paving the way for the subsequent application of feature selection techniques to enhance model performance and interpretability."
      ],
      "metadata": {
        "id": "5n1hoqetL6Sy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8C7GAssA4fw"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.feature_selection import SelectKBest, chi2, RFE, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define a function to evaluate the model\n",
        "def evaluate_model(X_train, X_test, y_train, y_test):\n",
        "    model = LogisticRegression(max_iter=10000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = recall_score(y_test, y_pred)\n",
        "    return score\n",
        "\n",
        "print(\"Baseline Model Performance:\")\n",
        "print(evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature selection using a Filter Method**\n",
        "The following code uses the `SelectKBest` function from the `sklearn.feature_selection` library in Python. This function is used to select the best features from a dataset based on a univariate statistical test.\n",
        "In this case, the code is using the `mutual_info_classif` test to select the 10 best features from the `X_train_scaled` and `X_test_scaled` datasets. The selected features are then stored in the `X_train_k_best` and `X_test_k_best` variables, respectively.\n",
        "The `mutual_info_classif` test measures the mutual information between each feature and the target variable. Mutual information is a measure of how much information one variable contains about another variable. In this case, the mutual information test is used to select the features that are most likely to be relevant to the target variable.\n",
        "The `SelectKBest` function can be used to improve the performance of a machine learning model by reducing the number of features in the dataset. This can make the model more efficient and less likely to overfit the data."
      ],
      "metadata": {
        "id": "w1vWHxMdRcii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Filter Method ###\n",
        "# Imports\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "print(\"\\n### Filter Method ###\")\n",
        "# Select the best features based on univariate statistical tests\n",
        "\n",
        "# Create a SelectKBest object using the mutual_info_classif test and select 10 features\n",
        "select_k_best = SelectKBest(mutual_info_classif, k=10)\n",
        "\n",
        "# Fit the SelectKBest object on the training data\n",
        "X_train_k_best = select_k_best.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "# Transform the test data using the fitted SelectKBest object\n",
        "X_test_k_best = select_k_best.transform(X_test_scaled)\n",
        "\n",
        "print(len(feature_names[select_k_best.get_support()]), \"Selected Features:\", feature_names[select_k_best.get_support()])\n",
        "print(\"Model Accuracy with Selected Features:\")\n",
        "print(evaluate_model(X_train_k_best, X_test_k_best, y_train, y_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "SAbN2QXCQzEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature selection using a Wrapper Method**\n",
        "\n",
        "The following code uses the RFE function from the sklearn.`feature_selection` library in Python. This function is used to select the best features from a dataset using a wrapper method.\n",
        "In this case, the code is using the `LogisticRegression` model as the estimator and selecting the 10 best features from the `X_train_scaled` and `X_test_scaled` datasets. The selected features are then stored in the `X_train_rfe` and `X_test_rfe` variables, respectively.\n",
        "The RFE function works by recursively fitting the model on the dataset and removing the least important features until the desired number of features is reached. The importance of each feature is determined by the model's coefficient."
      ],
      "metadata": {
        "id": "eib4n0Urb-vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Wrapper Method ###\n",
        "# Import the RFE function\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "print(\"\\n### Wrapper Method ###\")\n",
        "# Use Recursive Feature Elimination\n",
        "# Create a LogisticRegression object\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Create an RFE object using the LogisticRegression model and select 10 features\n",
        "rfe = RFE(estimator=model, n_features_to_select=10)\n",
        "\n",
        "# Fit the RFE object on the training data\n",
        "X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "# Transform the test data using the fitted RFE object\n",
        "X_test_rfe = rfe.transform(X_test_scaled)\n",
        "\n",
        "print(len(feature_names[rfe.get_support()]), \"Selected Features:\", feature_names[rfe.get_support()])\n",
        "print(\"Model Accuracy with RFE Selected Features:\")\n",
        "print(evaluate_model(X_train_rfe, X_test_rfe, y_train, y_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ln0-Ul8cRgud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature selection with an embedded method**\n",
        "\n",
        "This code demonstrates how to use LASSO regression with cross-validation to select features and subsequently evaluate a model's performance using the selected feature subset. The selected features are based on the non-zero coefficients obtained from the LASSO model.\n",
        "\n",
        "First, we import the `LassoCV` class from scikit-learn's linear model module. This class is a cross-validated version of LASSO regression. Next, we create a LASSO model (`lasso`) using cross-validation (`cv=5`). The max_iter parameter specifies the maximum number of iterations for the optimization algorithm. The model is fitted to the training data (`X_train_scaled`, `y_train`). Finally, we identify the features with non-zero coefficients, indicating that they were selected by the LASSO model. The threshold 1e-05 is used to consider coefficients with absolute values greater than this threshold as non-zero."
      ],
      "metadata": {
        "id": "lcH9mWTvo5r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Embedded Method ###\n",
        "from sklearn.linear_model import LassoCV\n",
        "print(\"\\n### Embedded Method ###\")\n",
        "\n",
        "# Fit the Lasso model with cross-validation to find the best alpha (regularization strength)\n",
        "lasso = LassoCV(cv=5, random_state=42, max_iter=10000).fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the coefficients\n",
        "coef = lasso.coef_\n",
        "print(coef)\n",
        "# Identify the non-zero coefficients (selected features)\n",
        "selected_features = np.where(np.abs(coef)>1e-05)[0]\n",
        "\n",
        "# Select data for non-zero features\n",
        "X_train_lasso = X_train_scaled[:, selected_features]\n",
        "X_test_lasso = X_test_scaled[:, selected_features]\n",
        "\n",
        "print(len(feature_names[selected_features]), \"Selected Features:\", feature_names[selected_features])\n",
        "print(\"Model Accuracy with LASSO Selected Features:\")\n",
        "print(evaluate_model(X_train_lasso, X_test_lasso, y_train, y_test))"
      ],
      "metadata": {
        "id": "XJ78RpyDcJwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Hybrid Methods**\n",
        "Hybrid methods that combine elements of both filter and wrapper methods are often employed to harness the benefits of both approaches. These hybrid methods aim to leverage the efficiency of filter methods in quickly preselecting relevant features and the model-awareness of wrapper methods in fine-tuning the feature subsets. Here are a few hybrid methods:\n",
        "\n",
        "* **Filter-Wrapper Hybrid**:\n",
        " * **Approach:** Initially, a filter method is used to preselect a subset of features based on some statistical measure (e.g., correlation, information gain). Then, a wrapper method is applied to fine-tune the selected subset using a predictive model.\n",
        " * **Benefits:** This approach aims to capitalize on the speed of filter methods for an initial feature selection and the model-awareness of wrapper methods to consider feature interactions.\n",
        "* **Filter-Embedded Hybrid:**\n",
        " * **Approach:** Filter methods are used to preselect a subset of features based on some criteria. The selected features are then incorporated into an embedded method during model training. For example, features identified by a filter method may be used in LASSO regression.\n",
        " * **Benefits:** This hybrid approach combines the efficiency of filter methods with the model-awareness of embedded methods, allowing for simultaneous feature selection and model training.\n",
        "* **Wrapper-Filter Hybrid:**\n",
        " * **Approach:** Similar to the filter-wrapper hybrid, a wrapper method is used to iteratively search for the best subset of features. Within each iteration, a filter method is applied to quickly evaluate feature subsets.\n",
        " * **Benefits:** This hybrid method aims to strike a balance between the exhaustive search of wrapper methods and the efficiency of filter methods, potentially reducing the computational cost.\n",
        "* **Filter-Wrapper-Embedded Hybrid:**\n",
        " * **Approach:** This comprehensive hybrid method combines all three approaches. It starts with a filter method for initial feature selection, employs a wrapper method for further refinement, and integrates embedded methods during model training.\n",
        " * **Benefits:** By combining filter, wrapper, and embedded methods, this approach aims to capture the advantages of each, providing a well-rounded feature selection process.\n",
        "\n",
        "Implementing hybrid methods requires careful consideration of the dataset characteristics, computational resources, and the specific goals of feature selection. The choice of hybrid method can be tailored based on the problem at hand and the trade-offs between computational efficiency and model interpretability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8B_Lk3fetHjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activity: Hybrid Method**\n",
        "\n",
        "In the Cancer Database, apply Mutual Information as a filter method to select the top k features. Then, use Recursive Feature Elimination (RFE) with Logistic Regression as the wrapper method on the selected features from Mutual Information. Finally, train a Logistic Regression model on the features selected by the filter-wrapper hybrid method and evaluate its performance on the test set."
      ],
      "metadata": {
        "id": "V_b91dj4vjaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Step 1: Select top k features using Mutual Information as a filter method\n",
        "k = 10  # Number of features to select\n",
        "select_k_best = SelectKBest(mutual_info_classif, k=k)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_mutual_info = select_k_best.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "# Get the indices of selected features\n",
        "selected_indices = select_k_best.get_support(indices=True)\n",
        "\n",
        "# Step 2: Apply Recursive Feature Elimination (RFE) with Logistic Regression as the wrapper method\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "rfe = RFE(estimator=model, n_features_to_select=k)\n",
        "\n",
        "# Fit RFE on the selected features from Mutual Information\n",
        "X_train_hybrid = rfe.fit_transform(X_train_mutual_info, y_train)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model on the features selected by the filter-wrapper hybrid method\n",
        "model_hybrid = LogisticRegression(max_iter=10000)\n",
        "model_hybrid.fit(X_train_hybrid, y_train)\n",
        "\n",
        "# Step 4: Evaluate the performance of the model on the test set\n",
        "# Transform test data using the selected features\n",
        "X_test_mutual_info = select_k_best.transform(X_test_scaled)\n",
        "X_test_hybrid = rfe.transform(X_test_mutual_info)  # Use the same RFE instance to transform test data\n",
        "\n",
        "# Evaluate model performance\n",
        "def evaluate_model_performance(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "\n",
        "    return {'Accuracy': accuracy, 'F1 Score': f1, 'Recall': recall, 'ROC AUC': roc_auc, 'Precision': precision}\n",
        "\n",
        "# Evaluate performance of hybrid model\n",
        "hybrid_performance = evaluate_model_performance(model_hybrid, X_test_hybrid, y_test)\n",
        "\n",
        "print(\"Performance of Filter-Wrapper Hybrid Model:\")\n",
        "print(hybrid_performance)"
      ],
      "metadata": {
        "id": "97eUzgmzyVTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}